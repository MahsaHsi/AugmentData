{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialization**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3roIer--jJmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmoQFRHr5VjE"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "READ_DATA_ONLINE = False\n",
        "\n",
        "RUNTIME_TYPE = 'COLAB'\n",
        "EXPERIMENT_NAME = 'test'\n",
        "K_FOLD = 5\n",
        "REPEAT_TIME = 1 #4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "wTL9z0eOykG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lRSfnpynFo",
        "outputId": "8ec2f9fb-186c-4d1e-b3d8-28edec4544e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkrRthosQlYZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "from nltk.corpus import wordnet\n",
        "from os import chdir\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTv-Br0CGVrv"
      },
      "outputs": [],
      "source": [
        "# To assure deterministic results\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mE5XjO_jORe"
      },
      "outputs": [],
      "source": [
        "# Support for third-party widgets\n",
        "if RUNTIME_TYPE == 'COLAB':\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "    from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "m4Eex09wjcoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "aXbeI7Zv1nTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_heads(text, annots):\n",
        "    heads = []\n",
        "    for item in annots:\n",
        "        heads.extend(text[item[2]:item[3]+1])\n",
        "        \n",
        "    return heads"
      ],
      "metadata": {
        "id": "fokpoqgCN4OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_synonym(split_text, split_heads):\n",
        "    arr_synonym = []\n",
        "    dict_synonym = {}\n",
        "    th_text = 1\n",
        "    th_word = 1\n",
        "\n",
        "    if len(split_text) > th_text:\n",
        "        for i in range(len(split_text)):\n",
        "            word = split_text[i]\n",
        "            if not(word in split_heads):\n",
        "                if len(word) > th_word:\n",
        "                    synonyms = wordnet.synsets(word)\n",
        "                    lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
        "                    if len(lemmas) > 0:\n",
        "                        dict_synonym[str(i)] = []\n",
        "                        for item in lemmas:\n",
        "                            if item != word and item.find('_') == -1 and len(item) > th_word and not(item.isupper()): #and len(item) == len(word):\n",
        "                                dict_synonym[str(i)].append(item)\n",
        "\n",
        "        for rp in range(REPEAT_TIME):\n",
        "            temp_text = split_text.copy()\n",
        "            for i in range(len(split_text)):\n",
        "                if str(i) in dict_synonym.keys():\n",
        "                    if rp < len(dict_synonym[str(i)]):\n",
        "                        temp_text[i] = (dict_synonym[str(i)])[rp]\n",
        "\n",
        "            if temp_text != split_text:\n",
        "                if not(temp_text in arr_synonym):\n",
        "                    arr_synonym.append(temp_text)\n",
        "\n",
        "    return arr_synonym"
      ],
      "metadata": {
        "id": "0-Xezndx2r5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read data**"
      ],
      "metadata": {
        "id": "8Au6Zj8Ujlb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqm_LbMq_T9Z"
      },
      "outputs": [],
      "source": [
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set destination folder\n",
        "if RUNTIME_TYPE == 'COLAB':\n",
        "  drive.mount('/content/drive')\n",
        "  if not os.path.exists('drive/MyDrive/augment-chinese-dataset-newV'):\n",
        "    os.makedirs('drive/MyDrive/augment-chinese-dataset-newV')\n",
        "  chdir('drive/MyDrive/augment-chinese-dataset-newV')\n",
        "else:\n",
        "  if not os.path.exists('augment-chinese-dataset-newV'):\n",
        "    os.makedirs('augment-chinese-dataset-newV')\n",
        "  chdir('augment-chinese-dataset-newV')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzZCJH_p2MnS",
        "outputId": "36f7c8ef-4d69-41d8-9e41-237bf05597b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7IzSDGM6NNS"
      },
      "outputs": [],
      "source": [
        "# Dataset public URL\n",
        "data_name_to_google_drive_url = {\n",
        "    '0.aaai19srl.train0.conll.json': 'https://drive.google.com/file/d/1-_eoQ3uoOBYj8cUum8mNa5wZtl6QBgOi/view?usp=share_link',\n",
        "    '0.aaai19srl.dev0.conll.json': 'https://drive.google.com/file/d/1-fsiHQGfpScZv6XMAmlDgnvWzAEqZGjv/view?usp=share_link',\n",
        "    '0.aaai19srl.test0.conll.json': 'https://drive.google.com/file/d/1-gw0li7UF-LxW4XoqrHvjIFmh2-7IqPa/view?usp=share_link',\n",
        "\n",
        "    '1.aaai19srl.train1.conll.json':'https://drive.google.com/file/d/1-XZOXPKhuhoG3LlSVMNp78aOmYeG9hNa/view?usp=share_link',\n",
        "    '1.aaai19srl.dev1.conll.json':'https://drive.google.com/file/d/1-e_3D3wSKe5PKXQRimTa99V7S0uI-3cV/view?usp=share_link',\n",
        "    '1.aaai19srl.test1.conll.json':'https://drive.google.com/file/d/1-dNGqiBPaMIEGP5ga08s2GRfWo99UX5C/view?usp=share_link',\n",
        "\n",
        "    '2.aaai19srl.train2.conll.json': 'https://drive.google.com/file/d/1-nCWw4by4KmbFuQ51_4Cp7gtd2MOFwcj/view?usp=share_link',\n",
        "    '2.aaai19srl.dev2.conll.json': 'https://drive.google.com/file/d/1-pKju4UG04NrjTV6sySPWXoNljUbrWcJ/view?usp=share_link',\n",
        "    '2.aaai19srl.test2.conll.json': 'https://drive.google.com/file/d/1-lA8KFec-ZJVuEVN5Odp7mtC2_DAOy2W/view?usp=share_link',\n",
        "\n",
        "    '3.aaai19srl.train3.conll.json': 'https://drive.google.com/file/d/1-jcqqZGYMZdUOcKdWqrqt8L0Jfq6Rb-T/view?usp=share_link',\n",
        "    '3.aaai19srl.dev3.conll.json': 'https://drive.google.com/file/d/1-kKx2l42_AJ09ZMTsNH35Gvn-4cCivh0/view?usp=share_link',\n",
        "    '3.aaai19srl.test3.conll.json': 'https://drive.google.com/file/d/1-pYnyOfepaq0vkxAA5gr13kgF_Gq-suV/view?usp=share_link',\n",
        "\n",
        "    '4.aaai19srl.train4.conll.json': 'https://drive.google.com/file/d/1-dpskoeMgcz-s_F1eEiDEvsyfofaRtKK/view?usp=share_link',\n",
        "    '4.aaai19srl.dev4.conll.json': 'https://drive.google.com/file/d/1-eh7dY7vNY34c_ZSTYNeWUrrI7-gLXbb/view?usp=share_link',\n",
        "    '4.aaai19srl.test4.conll.json': 'https://drive.google.com/file/d/1-ZzlosdvROC-kqphi44q1loowRi69uSh/view?usp=share_link'\n",
        "}\n",
        "\n",
        "# Get direct download link\n",
        "def get_download_url_from_google_drive_url(google_drive_url):\n",
        "    return f'https://drive.google.com/uc?id={google_drive_url.split(\"/\")[5]}&export=download&confirm=t'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add Synonym(s)**"
      ],
      "metadata": {
        "id": "I23TXZZKkYPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_all_folds = {}\n",
        "\n",
        "for k in range(1, K_FOLD+1):\n",
        "    print('fold: ', k)\n",
        "    train_data_name = f'{k-1}.aaai19srl.train{k-1}.conll.json'\n",
        "    dev_data_name = f'{k-1}.aaai19srl.dev{k-1}.conll.json'\n",
        "    test_data_name = f'{k-1}.aaai19srl.test{k-1}.conll.json'\n",
        "\n",
        "    # Extract train data samples\n",
        "    google_drive_url = data_name_to_google_drive_url[train_data_name]\n",
        "    data_url = get_download_url_from_google_drive_url(google_drive_url)\n",
        "    response = urlopen(data_url)\n",
        "    dd = response.readlines()\n",
        "    train_data = []\n",
        "\n",
        "    for line in dd:\n",
        "        train_data.append(json.loads(line.decode()))\n",
        "    print(f'Size of train data: {len(train_data)}')\n",
        "\n",
        "    for item in train_data:\n",
        "        sentence = item['sentences']\n",
        "        annots = item['orl']\n",
        "        if not str([sentence, annots]) in train_data_all_folds:\n",
        "            train_data_all_folds[str([sentence, annots])] = item\n",
        "\n",
        "    # Extract dev data samples\n",
        "    google_drive_url = data_name_to_google_drive_url[dev_data_name]\n",
        "    data_url = get_download_url_from_google_drive_url(google_drive_url)\n",
        "    response = urlopen(data_url)\n",
        "    dd = response.readlines()\n",
        "    dev_data = []\n",
        "\n",
        "    for line in dd:\n",
        "        dev_data.append(json.loads(line))\n",
        "    print(f'Size of dev data: {len(dev_data)}')\n",
        "\n",
        "    # Extract test data samples\n",
        "    google_drive_url = data_name_to_google_drive_url[test_data_name]\n",
        "    data_url = get_download_url_from_google_drive_url(google_drive_url)\n",
        "    response = urlopen(data_url)\n",
        "    dd = response.readlines()\n",
        "    test_data = []\n",
        "    \n",
        "    for line in dd:\n",
        "        test_data.append(json.loads(line))\n",
        "\n",
        "    print(f'Size of test data: {len(test_data)}')\n",
        "    print(f'All: {len(train_data)+len(dev_data)+len(test_data)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBL9bi4taAH0",
        "outputId": "1db67f0d-3800-4416-be45-3bfef4b53ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold:  1\n",
            "Size of train data: 2449\n",
            "Size of dev data: 1038\n",
            "Size of test data: 625\n",
            "All: 4112\n",
            "\n",
            "fold:  2\n",
            "Size of train data: 2455\n",
            "Size of dev data: 1038\n",
            "Size of test data: 628\n",
            "All: 4121\n",
            "\n",
            "fold:  3\n",
            "Size of train data: 2462\n",
            "Size of dev data: 1038\n",
            "Size of test data: 623\n",
            "All: 4123\n",
            "\n",
            "fold:  4\n",
            "Size of train data: 2553\n",
            "Size of dev data: 1038\n",
            "Size of test data: 532\n",
            "All: 4123\n",
            "\n",
            "fold:  5\n",
            "Size of train data: 2396\n",
            "Size of dev data: 1038\n",
            "Size of test data: 687\n",
            "All: 4121\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data_all_folds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awfizeItlX6Y",
        "outputId": "30772b3a-2caf-4014-e4a8-88659c64c59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find parts of data elements that should be saved, then call synonym method for add synonym(s) for each element\n",
        "train_data_after_synonym = {}\n",
        "\n",
        "for item in train_data_all_folds:\n",
        "    text = (train_data_all_folds[item])['sentences']\n",
        "    annots = (train_data_all_folds[item])['orl']\n",
        "    heads = get_heads(text, annots)\n",
        "    arr_synonym = add_synonym(text, heads)\n",
        "\n",
        "    if not (str([text, annots]) in train_data_after_synonym):\n",
        "        train_data_after_synonym[str([text, annots])] = []\n",
        "        train_data_after_synonym[str([text, annots])].append(train_data_all_folds[item])\n",
        "        \n",
        "    if arr_synonym != []:\n",
        "        for rp in range(REPEAT_TIME):\n",
        "            if rp < len(arr_synonym):\n",
        "                train_data_after_synonym[str([text, annots])].append({'sentences': arr_synonym[rp], 'orl': annots})"
      ],
      "metadata": {
        "id": "16AAswrAF-0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save data**"
      ],
      "metadata": {
        "id": "LbYycbV7j2ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(1, K_FOLD+1):\n",
        "    print('\\nfold: ', k)\n",
        "    train_data_name = f'{k-1}.aaai19srl.train{k-1}.conll.json'\n",
        "   \n",
        "    # Extract dev data samples\n",
        "    google_drive_url = data_name_to_google_drive_url[train_data_name]\n",
        "    data_url = get_download_url_from_google_drive_url(google_drive_url)\n",
        "    response = urlopen(data_url)\n",
        "    dd = response.readlines()\n",
        "    train_data = []\n",
        "    train_data_new = []\n",
        "\n",
        "    for line in dd:        \n",
        "        train_data.append(json.loads(line))\n",
        "    print(f'Size of trainset: {len(train_data)}')\n",
        "\n",
        "    for item in train_data:\n",
        "        sentence = item['sentences']\n",
        "        annots = item['orl']\n",
        "        for item in train_data_after_synonym[str([sentence, annots])]:\n",
        "            train_data_new.append(item)\n",
        "    print(f'Size of new trainset: {len(train_data_new)}')\n",
        "\n",
        "    with open(train_data_name, 'w') as fp:\n",
        "        fp.write(\n",
        "        '\\n'.join(json.dumps(item) for item in train_data_new) +\n",
        "        '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9L3AlyKYUqy",
        "outputId": "4fa34abf-8e19-40c1-cde7-2888af5ebf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fold:  1\n",
            "Size of trainset: 2449\n",
            "Size of new trainset: 4707\n",
            "\n",
            "fold:  2\n",
            "Size of trainset: 2455\n",
            "Size of new trainset: 4726\n",
            "\n",
            "fold:  3\n",
            "Size of trainset: 2462\n",
            "Size of new trainset: 4733\n",
            "\n",
            "fold:  4\n",
            "Size of trainset: 2553\n",
            "Size of new trainset: 4909\n",
            "\n",
            "fold:  5\n",
            "Size of trainset: 2396\n",
            "Size of new trainset: 4619\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}