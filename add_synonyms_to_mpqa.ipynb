{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialization**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3roIer--jJmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5Kp40wePVaI"
      },
      "outputs": [],
      "source": [
        "VERSION = 'MPQA2.0_v221219_cleaned'\n",
        "DATA = VERSION + 'addedSpanToHead'\n",
        "SAVE_NAME = DATA +'_addedSynonyms'\n",
        "SEED = 0\n",
        "\n",
        "RUNTIME_TYPE = 'COLAB'\n",
        "EXPERIMENT_NAME = 'test'\n",
        "REPEAT_TIME = 1 #4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "wTL9z0eOykG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lRSfnpynFo",
        "outputId": "9e730a5b-c33a-452c-9433-6f5e624479cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkrRthosQlYZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "from nltk.corpus import wordnet\n",
        "from os import chdir\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTv-Br0CGVrv"
      },
      "outputs": [],
      "source": [
        "# To assure deterministic results\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mE5XjO_jORe"
      },
      "outputs": [],
      "source": [
        "# Support for third-party widgets\n",
        "if RUNTIME_TYPE == 'COLAB':\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "    from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "m4Eex09wjcoY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITQmHUjb_Pn4"
      },
      "outputs": [],
      "source": [
        "def set_seed():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_heads(csds_object, text, head):\n",
        "    heads = []\n",
        "    if head.replace(' ', '') != '':\n",
        "        heads.append(head)\n",
        "\n",
        "    # nested source\n",
        "    nested_sources = csds_object['nested_source']\n",
        "    if len(nested_sources) > 0:\n",
        "        i = len(nested_sources) - 1\n",
        "        while i >= 0:\n",
        "            if len(nested_sources[i].keys()) == 0:\n",
        "                break\n",
        "            if i == 0:\n",
        "                if len(nested_sources[i].keys()) == 0:\n",
        "                    break\n",
        "                nslink = csds_object['nested_source_link'][i]\n",
        "                if nslink.split('&&')[1] == 'agent-w':\n",
        "                    break\n",
        "            if nested_sources[i]['clean_head'].replace(' ', '') != '':\n",
        "                heads.append(nested_sources[i]['clean_head'])\n",
        "            i = i - 1\n",
        "    \n",
        "    # target\n",
        "    target = csds_object['target']\n",
        "    if len(target) > 0:\n",
        "        i = len(target) - 1\n",
        "        while i >= 0:\n",
        "            if len(target[i].keys()) == 0:\n",
        "                break\n",
        "            if i == 0:\n",
        "                if len(target[i].keys()) == 0:\n",
        "                    break\n",
        "            if target[i]['clean_head'].replace(' ', '') != '':\n",
        "                heads.append(target[i]['clean_head'])\n",
        "            i = i - 1\n",
        "    \n",
        "    # attitude\n",
        "    attitude = csds_object['attitude']\n",
        "    if len(attitude) > 0:\n",
        "        i = len(attitude) - 1\n",
        "        while i >= 0:\n",
        "            if len(attitude[i].keys()) == 0:\n",
        "                break\n",
        "            if i == 0:\n",
        "                if len(attitude[i].keys()) == 0:\n",
        "                    break\n",
        "            if attitude[i]['clean_head'].replace(' ', '') != '':\n",
        "                heads.append(attitude[i]['clean_head'])\n",
        "            i = i - 1\n",
        "\n",
        "    return heads"
      ],
      "metadata": {
        "id": "_uZAEQcEpbTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_synonym(clean_text, heads):\n",
        "    arr_synonym = []\n",
        "    str_heads = ''\n",
        "    dict_synonym = {}\n",
        "    th_text = 1 \n",
        "    th_word = 1\n",
        "\n",
        "    split_text = clean_text.split(' ')\n",
        "\n",
        "    if len(split_text) > th_text:\n",
        "        for i in range(len(heads)):\n",
        "            str_heads += heads[i]\n",
        "            if i <= len(heads)-2:\n",
        "                str_heads += ' '\n",
        "\n",
        "        split_heads = str_heads.split(' ')\n",
        "\n",
        "        for i in range(len(split_text)):\n",
        "            word = split_text[i]\n",
        "            if not(word in split_heads):\n",
        "                if len(word) > th_word:\n",
        "                    synonyms = wordnet.synsets(word)\n",
        "                    lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
        "                    if len(lemmas) > 0:\n",
        "                        dict_synonym[str(i)] = []\n",
        "                        for item in lemmas:\n",
        "                            if item != word and item.find('_') == -1 and len(item) > th_word and not(item.isupper()): #and len(item) == len(word):\n",
        "                                dict_synonym[str(i)].append(item)\n",
        "\n",
        "        for rp in range(REPEAT_TIME):\n",
        "            temp_text = split_text.copy()\n",
        "            for i in range(len(split_text)):\n",
        "                if str(i) in dict_synonym.keys():\n",
        "                    if rp < len(dict_synonym[str(i)]):\n",
        "                        temp_text[i] = (dict_synonym[str(i)])[rp]\n",
        "\n",
        "            if temp_text != split_text:\n",
        "                temp_arr = ' '.join(temp_text)\n",
        "                if not(temp_arr in arr_synonym):\n",
        "                    arr_synonym.append(temp_arr)\n",
        "\n",
        "    return arr_synonym"
      ],
      "metadata": {
        "id": "uhbpO242-Wf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read data**"
      ],
      "metadata": {
        "id": "8Au6Zj8Ujlb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqm_LbMq_T9Z"
      },
      "outputs": [],
      "source": [
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgDnxHr3jPDn",
        "outputId": "3863c2e1-3f5c-4ea5-f21f-52bc9d867f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set destination folder\n",
        "if RUNTIME_TYPE == 'COLAB':\n",
        "  drive.mount('/content/drive')\n",
        "  if not os.path.exists('drive/MyDrive/new-csds-newV'):\n",
        "    os.makedirs('drive/MyDrive/new-csds-newV')\n",
        "  chdir('drive/MyDrive/new-csds-newV')\n",
        "else:\n",
        "  if not os.path.exists('new-csds-newV'):\n",
        "    os.makedirs('new-csds-newV')\n",
        "  chdir('new-csds-newV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFhRRXO3NvV7"
      },
      "outputs": [],
      "source": [
        "# Getting data & augmented data urls\n",
        "data_name_to_google_drive_data_url = {\n",
        "    'MPQA2.0_v221219_cleanedaddedSpanToHead': 'https://drive.google.com/file/d/1cWzWDNScc1QOCH1ojJaY0wW4oPzVKSn1/view?usp=share_link'\n",
        "}\n",
        "\n",
        "# Get direct download link\n",
        "def get_download_url_from_google_drive_url(google_drive_url):\n",
        "    return f'https://drive.google.com/uc?id={google_drive_url.split(\"/\")[5]}&export=download&confirm=t'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gLjikSSNsY-"
      },
      "outputs": [],
      "source": [
        "# Read MPQA data\n",
        "google_drive_data_url = data_name_to_google_drive_data_url[DATA]\n",
        "data_url = get_download_url_from_google_drive_url(google_drive_data_url)\n",
        "response = urlopen(data_url)\n",
        "csds_collection = json.loads(response.read())\n",
        "csds_objects = csds_collection['csds_objects']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = {}\n",
        "new_data['corpus_name'] = csds_collection['corpus_name']\n",
        "new_data['agent_objects'] = csds_collection['agent_objects']\n",
        "new_data['target_objects'] = csds_collection['target_objects']"
      ],
      "metadata": {
        "id": "9XivAtRANB2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add Synonym(s)**"
      ],
      "metadata": {
        "id": "I23TXZZKkYPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzHqWSmnxeYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd70df6-2f27-46cd-8e86-6f0a74e1339c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n"
          ]
        }
      ],
      "source": [
        "# Find parts of csds elements that should be saved (clean head of text, nested source, target, agent), then call synonym method for add synonym(s) for each element\n",
        "new_csds_objects = []\n",
        "counter = 0\n",
        "\n",
        "for csds_object in csds_objects:\n",
        "    text = csds_object['clean_text']\n",
        "    head = csds_object['clean_head']\n",
        "\n",
        "    if text.replace(' ', '') != '':\n",
        "        heads = get_heads(csds_object, text, head)\n",
        "        csds_object['synonyms'] = add_synonym(text, heads)\n",
        "    else:\n",
        "        csds_object['synonyms'] = []\n",
        "        counter += 1\n",
        "\n",
        "    # Add to new dictionary\n",
        "    new_csds_objects.append(csds_object)\n",
        "\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new csds objects\n",
        "new_data['csds_objects'] = new_csds_objects\n",
        "\n",
        "del new_csds_objects, csds_objects"
      ],
      "metadata": {
        "id": "7DdEw4ggMev4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save data**"
      ],
      "metadata": {
        "id": "LbYycbV7j2ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(SAVE_NAME+'.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(new_data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "RcE53CedGvsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}