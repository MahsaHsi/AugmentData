{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialization**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3roIer--jJmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N5Kp40wePVaI"
      },
      "outputs": [],
      "source": [
        "REPEAT_TIME = 4 # Number of times to augment the data (how many times?)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "wTL9z0eOykG9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lRSfnpynFo",
        "outputId": "3ecf151e-d1f7-4e86-e070-204980ac53b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LkrRthosQlYZ"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "m4Eex09wjcoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_synonym(text, sth_keep):\n",
        "    th_text, th_word = 1, 1 # Minimum acceptable length for text and word\n",
        "    arr_synonym = []\n",
        "    dict_synonym = {}\n",
        "\n",
        "    split_text = text.split(' ')\n",
        "\n",
        "    if len(split_text) > th_text:\n",
        "\n",
        "        sth_keep_str = ''\n",
        "        for i in range(len(sth_keep)):\n",
        "            sth_keep_str += sth_keep[i]\n",
        "            if i <= len(sth_keep)-2:\n",
        "                sth_keep_str += ' '\n",
        "\n",
        "        sth_keep_split = sth_keep_str.split(' ')\n",
        "\n",
        "        for i in range(len(split_text)):\n",
        "            word = split_text[i]\n",
        "\n",
        "            if not(word in sth_keep_split):\n",
        "                if len(word) > th_word:\n",
        "                    synonyms = wordnet.synsets(word)\n",
        "                    lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
        "                    if len(lemmas) > 0:\n",
        "                        dict_synonym[str(i)] = []\n",
        "                        for item in lemmas:\n",
        "                            if not(item in dict_synonym[str(i)]):\n",
        "                                if item != word and item.find('_') == -1 and len(item) > th_word and not(item.isupper()): #and len(item) == len(word):\n",
        "                                    dict_synonym[str(i)].append(item)\n",
        "\n",
        "        for rp in range(REPEAT_TIME):\n",
        "            temp_text = split_text.copy()\n",
        "            for i in range(len(split_text)):\n",
        "                if str(i) in dict_synonym.keys():\n",
        "                    if rp < len(dict_synonym[str(i)]):\n",
        "                        temp_text[i] = (dict_synonym[str(i)])[rp]\n",
        "\n",
        "            if temp_text != split_text:\n",
        "                temp_arr = ' '.join(temp_text)\n",
        "                if not(temp_arr in arr_synonym):\n",
        "                    arr_synonym.append(temp_arr)\n",
        "\n",
        "    return arr_synonym"
      ],
      "metadata": {
        "id": "uhbpO242-Wf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add Synonym(s)**"
      ],
      "metadata": {
        "id": "I23TXZZKkYPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tzHqWSmnxeYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "affddce5-5096-4014-e387-d72ede8acb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  A proposal for an early quota system will be at the center of this report.\n",
            "Parts of text that should be kept:  ['early quota system']\n",
            "--------------------------------------------------\n",
            "Results: \n",
            "A proposition for an early quota system volition cost At the marrow of this report.\n",
            "A proposal for an early quota system bequeath beryllium astatine the pore of this report.\n",
            "A proposal for an early quota system leave glucinium at the rivet of this report.\n",
            "A proposal for an early quota system testament Be at the essence of this report.\n"
          ]
        }
      ],
      "source": [
        "# Call synonym method for add synonym(s)\n",
        "text = 'A proposal for an early quota system will be at the center of this report.' # Main text\n",
        "sth_keep = ['early quota system'] # Parts of text that should be kept\n",
        "\n",
        "results = []\n",
        "\n",
        "if text.replace(' ', '') != '':\n",
        "    results = add_synonym(text, sth_keep)\n",
        "\n",
        "print('Text: ', text)\n",
        "print('Parts of text that should be kept: ', sth_keep)\n",
        "print('-'*50)\n",
        "print('Results: ')\n",
        "for res in results:\n",
        "    print(res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}